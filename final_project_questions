1)Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]


--The goal of this project is to determine, based upon information made public as a result of the Enron scandal, whether someone, who is included in the dataset, is a POI, or person of interest. In the data, a POI has a label with a value of 1.0 if the entry in the dataset is a POI, and a label with a value of 0.0 if the entry is not a POI. Machine Learning is useful in accomplishing this goal because several different possible machine learning algorithms can be used to take in many different possible combinations of features, which are data points characterizing each entry in the dataset, and give us a recommendation as to whether someone is a POI. The dataset is a combination of an email corpus and financials pertaining to 146 top executives who were employed by Enron. Through the careful examination of certain features pertaining to the data, we are able to use machine learning to predict with a fairly high degree of certainty whether one of these executives is a POI. 

To figure out if there were outliers, I plotted each of my features in a 2D plot aginst salary. My reasoning for this is salary is determined in a fairly conventional way: those at the top get the most, those in the middle get medium amounts, and those toward the bottom get the least. However, other things like bonus, stock options and other financial features could let us know if there is an outlier somewhere in the data. There did not appear to be any abnormal or invalid outliers in the data, except the total value entry for each of the features was included in the initial dataset. That was removed from the dataset, before proceeding forward with the rest of my outlier analysis. Also, the Travel Agency in the Park entry was removed as well, since it did not seem to be related in any way to the rest of the data.  



2)What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “properly scale features”, “intelligently select feature”]


--The features I ended up using were salary, total_payments, bonus, total_stock_value, and exercised_stock_options. I also included the feature I created, poi_emails. I chose these features because I thought to myself, "if a group of people were trying to rip off the country, through which features could they get their money the fastest?" I answered that question with the above features and it seemed to work pretty well. I included the feature I created because it didn't change the performance of the algorithm when it was included vs. when it wasn't. 

I created poi_emails because I thought there might be a helpful relationship between the total number of emails in the categories of from_this_person_to_poi, from_poi_to_this_person, and shared_receipt_with_poi. I figured some POIs might have gotten fewer in one of the features, but a higher amount in others. I used SelectKBest for feature selection. The parameters I used were chosen via a pipeline and GridSearchCV to determine the best SelectKbest parameter values. The feature scores and features used were {'bonus': 20.257184, 'total_stock_value': 23.6137404, "exercised_stock_options: 24.250472"}. The parameter values for SelectKBest, as determined by GridSearchCV, ended up being k = 3 and score_func = chi2. 


3) What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]


--The algorithm I ended up using is SVC. The other ones I tried are SVC and DecisionTreeClassifier. SVC performance was pretty poor spitting out very low precision and recall scores. Decisiontree performance was slightly below the GaussianNB performance. One such iteration of the decisiontreeclassifier was able to attain .28 on precision, and a recall of .33. GaussianNB had steady performance across all cross validation scoring parameters and estimator parameters per GridSearchCV. 

4) What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  


--What it means to tune the parameters of an algorithm is to determine its settings. For example in the DecisionTreeClassifier, there are a couple parameters, min_samples_split and min_samples_leaf, which determine how large the decision tree will be. How large or small the decision tree is impacts its accuracy or performance. To tune the parameters of my algorithm, I used a pipeline and GridSearchCV to exhaustively search the best parameter combination for all of the algorithms that I tested. If one does not tune their algorithm well, its performance will be subpar.

5) What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric item: “validation strategy”]


--Validation is how we check the performance of our model. A classic mistake you can make if you do it wrong is assuming your model is really good, when in reality, its really bad. This can happen a couple of different ways. In a classification problem, like the one we are dealing with in this project, one could only use the accuracy metric when determining how well the algorithm predicts outcomes. The problem with this is it ignores other key metrics, mainly precision and recall. Our model may be great at predicting a negative case, when it truly is a negative case, and the same for a positive case. However, it may accomplish this by predicting a positive case when the outcome is a negative case (false positive), or predicting a negative case when the outcome is a positive case (false negative). The former reduces precision and the latter reduces recall. Another classic mistake is validating your model on the entire dataset instead of training on one part of the dataset and testing on another. 

I validated my analysis using StratifiedShuffleSplit, where all of the possible parameter combinations were run and cross validated, with the best performing parameter combination being fit on the data. StratifiedShuffleSplit uses different iterations of reshuffled data split into test and train sets, which then coupled with GridSearchCV determines the validated parameter tune with the highest scoring metrics. 

6) Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]


--My final algorithm performed the same when I included the feature I made, and when I didn't include it. The accuracy, precision and recall scores respectively are .85060, .42191, and .32550. My algorithm does a pretty good job at predicting a negative case when its a negative case, and predicting a positive case when its actually a positive case. However, it does output a semi-high amount of false alarms (false positives), and incorrect predictions (false negatives), judging from the precision and recall scores, which are both below .5. If precision and recall scores were higher, it would indicate our algorithm outputs fewer false alarms and incorrect predictions. 