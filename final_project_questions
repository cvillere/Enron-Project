1)Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]


--The goal of this project is to determine, based upon information made public as a result of the Enron scandal, whether someone, who is included in the dataset, is a POI, or person of interest. In the data, a POI has a label with a value of 1.0 if the entry in the dataset is a POI, and a label with a value of 0.0 if the entry is not a POI. Machine Learning is useful in accomplishing this goal because several different possible machine learning algorithms can be used to take in many different possible combinations of features, which are data points characterizing each entry in the dataset, and give us a recommendation as to whether someone is a POI. The dataset is a combination of an email corpus and financials pertaining to 146 top executives who were employed by Enron.
Of those, 18 are POIs and 128 are non POIs. In addition, there are several data values for each of the data points, which do not have any values. For example, of the 146 data points, a NaN (Not a Number) value is used 21 times in the Total_Payments category, 20 times in the Total_Stock_Value category, and 44 times in the Exercised_Stock_Options category. Through the careful examination of certain features pertaining to the data, we are able to use machine learning to predict with a fairly high degree of certainty whether one of these executives is a POI. 

To figure out if there were outliers, I plotted each of my features in a 2D plot aginst salary. My reasoning for this is salary is determined in a fairly conventional way: those at the top get the most, those in the middle get medium amounts, and those toward the bottom get the least. However, other things like bonus, stock options and other financial features could let us know if there is an outlier somewhere in the data. There did not appear to be any abnormal or invalid outliers in the data, except the total value entry for each of the features was included in the initial dataset. That was removed from the dataset, before proceeding forward with the rest of my outlier analysis. Also, the Travel Agency in the Park entry was removed as well, since it did not seem to be related in any way to the rest of the data.  



2)What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “properly scale features”, “intelligently select feature”]


--The features I ended up using were salary, total_payments, bonus, total_stock_value, and exercised_stock_options. I also included the feature I created, poi_emails. I chose these features because I thought to myself, "if a group of people were trying to rip off the country, through which features could they get their money the fastest?" I answered that question with the above features and it seemed to work pretty well. I included the feature I created because it didn't change the performance of the algorithm when it was included vs. when it wasn't. I did use feature scaling in my identifier. The reason why I used it is it had been mentioned in the videos SVM performance is increased when scaling is implemented, because it is able to blunt the effect outsized values have on the identifier's ability to parse non POIs from POIs. 

I created poi_emails because I thought there might be a helpful relationship between the total number of emails in the categories of from_this_person_to_poi, from_poi_to_this_person, and shared_receipt_with_poi. I figured some POIs might have gotten fewer in one of the features, but a higher amount in others. I used SelectKBest for feature selection. The parameters I used were chosen via a pipeline and GridSearchCV to determine the best SelectKbest parameter values. The feature scores and features used were {'bonus': 20.257184, 'total_stock_value': 23.6137404, "exercised_stock_options: 24.250472"}. The parameter values for SelectKBest, as determined by GridSearchCV, ended up being k = 3 and score_func = chi2. 


3) What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]


--The algorithm I ended up using is SVC. The other ones I tried are SVC and DecisionTreeClassifier. Decisiontree performance was slightly below the GaussianNB performance. One such iteration of the decisiontreeclassifier was able to attain .28 on precision, and a recall of .33. GaussianNB had steady performance across all cross validation scoring parameters and estimator parameters per GridSearchCV. 

4) What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  


--What it means to tune the parameters of an algorithm is to use trial and error to determine the best values for each of the alogorithm's parameters such that optimal performance is obtained. For example in the DecisionTreeClassifier, there are a couple parameters, min_samples_split and min_samples_leaf, which determine how large the decision tree will be, which in turn determines its accuracy, or performance. To tune the parameters of my algorithm, I used a pipeline and GridSearchCV to exhaustively search the best parameter combination for all of the algorithms that I tested. If one does not spend a good deal of time doing parameter tuning, they will be stuck with an algorithm, which does not perform as well as it could, or an algorithm that is overfitted to the data. 

5) What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric item: “validation strategy”]


--Validation is how we check the performance of our model. A classic mistake you can make if you do it wrong is assuming your model is really good, when in reality, its really bad. This can happen a couple of different ways. In a classification problem, like the one we are dealing with in this project, one could only use the accuracy metric when determining how well the algorithm predicts outcomes. The problem with this is it ignores other key metrics, mainly precision and recall. Our model may be great at predicting a negative case, when it truly is a negative case, and the same for a positive case. However, it may accomplish this by predicting a positive case when the outcome is a negative case (false positive), or predicting a negative case when the outcome is a positive case (false negative). The former reduces precision and the latter reduces recall. Another classic mistake is validating your model on the entire dataset instead of training on one part of the dataset and testing on another. 

I validated my analysis using StratifiedShuffleSplit. This is the best cross validation technique for this dataset because of its small size and small number of positive values, with there only being 18 POIs in the dataset. If we were to use a simple train/test split validator, it is possible the sets could be unbalanced with no values from one of the target classes in one of the splits. Using StratifiedShuffleSplit eliminates this issue, and adds a randomization component that is not in the StratifiedKFold cross validator.

6) Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]


--My final algorithm performed the same when I included the feature I made, and when I didn't include it. The accuracy, precision and recall scores respectively are .85060, .42191, and .32550. 
My algorithm makes a correct prediction 85% of the time. This does seem like a pretty good number; however, due to the small size of dataset, if the algorithm guessed non-POI everytime, it would have an accuracy score of .874. Due to this knowledge of the dataset, accuracy is not a dependable metric upon which we can determine how accurate our classifier is. We must also take into account
precision and recall. We want our classifer to have high precision and recall scores. Precision is a measure of how often a classifier's prediction is a false alarm. Recall is a measure of how often a classifier's prediction is just wrong. Higher precision and recall scores indicate fewer false alarms and incorrect predictions and more correct predictions. Lower precision and recall scores indicate more false alarms and incorrect predictions and fewer correct predictions. It is important to take all 3 of these scoring metrics into consideration when evaluating the performance of a classifier. Using just one metric could cause someone to think an algorithm performs better than it actually does.

My algorithm does a pretty good job of making correct predictions; however, the somewhat low 
precision and recall scores indicate it is telling me there is a POI way more frequently than there actually is, and its telling me there isn't a POI when there actually is way too frequently. 

